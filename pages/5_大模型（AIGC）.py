import streamlit as st
import os
from utils.load_info import load_info
from langchain_openai import OpenAI
from langchain_deepseek import ChatDeepSeek
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
# promptæ–‡ä»¶è·¯å¾„
prompt_root = os.path.join(project_root, 'data/Prompt')

# LangChain é…ç½®
os.environ["LANGCHAIN_TRACING"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "AIGC"
os.environ["LANGCHAIN_API_KEY"] = load_info("keys","LANGCHAIN_API_KEY")

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(page_title="AIGC èŠå¤©æœºå™¨äºº", page_icon="ğŸ¤–")

st.sidebar.info( 
    "æ¬¢è¿ä½¿ç”¨ AIGC èŠå¤©æœºå™¨äººï¼\n\n"
)

# è¯»å–promptæ–‡ä»¶
def read_prompt_file(dept):
    with open(os.path.join(prompt_root,f"{dept}.md"), "r") as file:
        return file.read()

st.sidebar.header("åŠ©æ‰‹é…ç½®")
selected_model = st.sidebar.selectbox("é€‰æ‹©å¤§æ¨¡å‹",("DeepSeek-Chat", "Medical_Qwen3"))
domain = st.sidebar.selectbox("é€‰æ‹©åŠ©æ‰‹é¢†åŸŸ",("é€šç”¨é¢†åŸŸ", "åŒ»ç–—åŠ©æ‰‹", "æ•™è‚²åŠ©æ‰‹", "æ³•å¾‹åŠ©æ‰‹"))

if domain == "åŒ»ç–—åŠ©æ‰‹":
    dept = st.sidebar.selectbox("é€‰æ‹©ç§‘å®¤", ("ç”·ç§‘", "å†…ç§‘", "å¦‡äº§ç§‘", "è‚¿ç˜¤ç§‘", "å„¿ç§‘", "å¤–ç§‘"))
    title = domain + " - " + dept
    # è¯»å–prompt
    system = read_prompt_file(dept)
else:
    title = domain
    system = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{domain}åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚"

# ä¾§è¾¹æ ä¿¡æ¯
st.sidebar.header("æ¨¡å‹é…ç½®")
if st.sidebar.checkbox("å¯ç”¨æµå¼è¾“å‡º", value=True):
    stream_output = True
else:
    stream_output = False

temperature = st.sidebar.slider("æ¸©åº¦", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
max_tokens = st.sidebar.slider("æœ€å¤§ç”ŸæˆTokenæ•°", min_value=10, max_value=2048, value=512, step=10)


st.sidebar.header("ä½¿ç”¨è¯´æ˜")
st.sidebar.info(
    "é€‰æ‹©ä¸€ä¸ªé¢†åŸŸåï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å‚æ•°ã€‚åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç„¶åç‚¹å‡»å‘é€æŒ‰é’®å³å¯å¼€å§‹å¯¹è¯ã€‚"
)


# è‡ªå®šä¹‰CSSï¼Œç¾åŒ–ç•Œé¢
st.markdown("""
<style>
    .stApp {
        background_color: #f0f2f6;
    }
    .stChatInputContainer > div {
        background-color: white;
    }
    [data-testid="stChatMessageContent"] {
        background-color: #e6e6fa;
        border-radius: 0.5rem;
        padding: 0.75rem;
    }
    [data-testid="stChatMessageContent"][aria-label="assistant message"] {
        background-color: #d1e7dd; /* æµ…ç»¿è‰²èƒŒæ™¯ */
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 0.5rem;
    }
    .stButton>button:hover {
        background-color: #45a049;
    }
    h1 {
        color: #333;
    }
</style>
""", unsafe_allow_html=True)

st.title(f"ğŸ’¬ AIGC {title}")
st.caption("Powered by AIE-52 G5")

# åˆå§‹åŒ–èŠå¤©è®°å½• (ä½¿ç”¨ session_state)
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯AIGC èŠå¤©æœºå™¨äººï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}
    ]

# æ˜¾ç¤ºèŠå¤©è®°å½•
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# è·å–æ¨¡å‹é…ç½®
model_info = load_info("models", selected_model)
print(f"æ¨¡å‹é…ç½®: {model_info}")
model_name = model_info["model_name"]
base_url = model_info["base_url"]
api_key = model_info["api_key"]

if selected_model == "DeepSeek-Chat":
    Model = ChatDeepSeek
else:
    Model = OpenAI

# å®šä¹‰æ¨¡å‹
llm = Model(
    model=model_name,
    base_url=base_url,
    api_key=api_key,
    temperature=temperature,
    max_tokens=max_tokens,
    streaming=stream_output
    )

# å®šä¹‰æç¤ºæ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", system),
    ("human", "{user_input}")
])

# åˆ›å»ºèŠå¤©é“¾
chain = prompt_template | llm | StrOutputParser()

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©è®°å½•
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # è·å–æœºå™¨äººå›å¤ (æ˜¾ç¤ºåŠ è½½çŠ¶æ€)
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            # åˆ›å»ºç©ºç™½å ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
            response_placeholder = st.empty()
            response_content = ""
            try:
                # ä½¿ç”¨LangChainå¤„ç†æ¶ˆæ¯å¹¶å®ç°æµå¼è¾“å‡º
                for chunk in chain.stream({"user_input": prompt}):
                    response_content += chunk
                    response_placeholder.markdown(response_content + "â–Œ")  # æ·»åŠ å…‰æ ‡æ•ˆæœ
                
                # å®Œæˆåæ˜¾ç¤ºæœ€ç»ˆå†…å®¹
                response_placeholder.markdown(response_content)
            except Exception as e:
                st.error(f"An error occurred: {e}")
                response_placeholder.markdown("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
    
    # å°†æœºå™¨äººå›å¤æ·»åŠ åˆ°èŠå¤©è®°å½•
    st.session_state.messages.append({"role": "assistant", "content": response_content})

# æ¸…ç©ºèŠå¤©è®°å½•æŒ‰é’®
if len(st.session_state.messages) > 1:
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = [
            {"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯AIGC èŠå¤©æœºå™¨äººï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}
        ]
        st.rerun()